{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pymorphy2\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1:  Сравнение стилей текстов\n",
    "### Выполнили:  Булгаков Дмитрий, Тефикова Алие\n",
    "### Группа ИАД-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составьте самостоятельно как минимум две коллекции\n",
    "текстов разных стилей (например, коллекция текстов в публицистическом\n",
    "стиле и коллекция текстов в научном стиле). Коллекции текстов\n",
    "должны быть достаточно большие (порядка 5000 токенов). Посчитайте\n",
    "количество токенов и типов в каждой коллекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Получение  данных из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_control_characters(text_string):\n",
    "    return ''.join(filter(None, text_string.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Загрузка художественных текстов (Портрет Дориана Грея,  Оскар Уайльд)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«Портре́т Дориана Гре́я» (англ. The Picture of Dorian Gray) — единственный опубликованный роман Оскара Уайльда. В жанровом отношении представляет смесь романа воспитания с моральной притчей. Существует в двух версиях — в 13 главах (1890 года) и в 20 главах (1891 года). Стал самым успешным произведением Уайльда, более 30 раз экранизировался."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link:<b> http://lib.ru/WILDE/doriangray.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fiction = open('data/dorian_gray.txt', encoding='utf-8').read()\n",
    "fiction = remove_control_characters(fiction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Загрузка  публицистических текстов (статьи lenta.ru) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenta.ru — одно из ведущих российских новостных интернет-изданий, основанное в 1999 году Антоном Носиком при содействии Фонда эффективной политики. Работает круглосуточно, освещая мировые и внутрироссийские новости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link<b>: http://lenta.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "journalistic = open('data/lentaru.txt', encoding='utf-8').read()\n",
    "journalistic = remove_control_characters(journalistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Загрузка  научных текстов (Молодежная Наука 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы Всероссийской научно-практической конференции молодых ученых, аспирантов и студентов, посвященной 150-летию со дня рождения профессора В.Н. Варгина (Пермь, 14-18 марта 2016 года)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link<b>: http://pgsha.ru/web/science/scientificarticles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scientific = open('data/perm_conf.txt', encoding='utf-8').read()\n",
    "scientific = remove_control_characters(scientific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Загрузка  текстов  разговорного стиля (корпус составленный на базе Twitter )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве источника текстов была выбрана платформа микроблогинга Twitter. Современные поисковые системы и имеющиеся в открытом доступе инструменты по сбору текстовых отзывов не позволяют собирать актуальные отзывы и оперативно работать с данными. В связи с этим на основе программного интерефейса API twitter  был разработан программный инструмент для извлечения отзывов об интересующих товарах, услугах,  событиях,  персонах из микроблоггинг-платформы twitter,  который позволяет учитывать время публикации сообщения и авторитетность автора сообщения. Этот инструмент использовался для сбора неразмеченного корпуса. В корпусе содержится более 15 миллионов записей за время с конца ноября 2013 года до конца февраля 2014 года."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link:<b> http://study.mokoron.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conversational = open('data/twitter.txt', encoding='utf-8').read()\n",
    "conversational = remove_control_characters(conversational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Подсчет токенов и типов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_symbols = set(punctuation + '0123456789'+u'–—'+u'«»'+u'“')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text, exlude_symb):\n",
    "    text = text.lower()\n",
    "    text_merged = ''.join(ch for ch in text if ch not in exlude_symb)\n",
    "    text_tokens = WhitespaceTokenizer().tokenize(text_merged.lower())\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(tokens):\n",
    "    print('N of tokens: ', len(tokens))\n",
    "    types = nltk.FreqDist(tokens)\n",
    "    print('N of types:', len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Токены и типы для  художественного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  60528\n",
      "N of types: 18236\n"
     ]
    }
   ],
   "source": [
    "fiction_tokens = tokenize(fiction, exclude_symbols)\n",
    "print_results(fiction_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Токены и типы для публицистического стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  13699\n",
      "N of types: 6015\n"
     ]
    }
   ],
   "source": [
    "journalistic_tokens = tokenize(journalistic, exclude_symbols)\n",
    "print_results(journalistic_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Токены и типы для  научного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  265376\n",
      "N of types: 57790\n"
     ]
    }
   ],
   "source": [
    "scientific_tokens = tokenize(scientific, exclude_symbols)\n",
    "print_results(scientific_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Токены и типы для  разговорного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  255367\n",
      "N of types: 56878\n"
     ]
    }
   ],
   "source": [
    "conversational_tokens = tokenize(conversational, exclude_symbols)\n",
    "print_results(conversational_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Подсчет частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя любой морфологический процессор, который вам нравится (pymorphy2, mystem), определите к какой части речиотносятся слова из каждой коллекции текстов. При помощи nltk.FreqDist() составьте частотные словари: часть речи – количество слов, к ней относящихся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 40435\n",
      "и 1914\n",
      "он 1773\n",
      "в 1585\n",
      "я 1477\n",
      "не 1211\n",
      "вы 913\n",
      "что 904\n",
      "быть 729\n",
      "на 725\n",
      "с 689\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "f_types = nltk.FreqDist(fiction_tokens)\n",
    "for t in c_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += f_types[t]\n",
    "        else:\n",
    "            lemmata[l] = f_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += f_types[t]\n",
    "        else:\n",
    "            lemmata[t] = f_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fiction_stops = stopwords.words('russian') + [u'это']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "весь 451\n",
      "человек 296\n",
      "который 282\n",
      "свой 277\n",
      "генри 243\n",
      "жизнь 225\n",
      "сказать 219\n",
      "мочь 196\n",
      "гарри 165\n",
      "ещё 152\n",
      "греть 141\n",
      "знать 134\n",
      "хотеть 130\n",
      "говорить 125\n",
      "рука 124\n",
      "глаз 122\n",
      "очень 117\n",
      "большой 110\n",
      "стать 107\n",
      "портрет 101\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in fiction_stops:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 40435\n",
      "я 10214\n",
      "не 7760\n",
      "в 6501\n",
      "и 6376\n",
      "на 4281\n",
      "что 3477\n",
      "с 3452\n",
      "а 3077\n",
      "весь 3017\n",
      "ты 2850\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "c_types = nltk.FreqDist(conversational_tokens)\n",
    "for t in c_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += c_types[t]\n",
    "        else:\n",
    "            lemmata[l] = c_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += c_types[t]\n",
    "        else:\n",
    "            lemmata[t] = c_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conversational_stops = stopwords.words('russian')+[u'это'+u'оо'+u'изз'+u'б']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "весь 3017\n",
      "это 2333\n",
      "ещё 1172\n",
      "хотеть 1125\n",
      "мочь 916\n",
      "день 855\n",
      "сегодня 790\n",
      "очень 733\n",
      "просто 669\n",
      "свой 663\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in conversational_stops:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scientific_stops = stopwords.words('russian')  + [u'№', u'это', u'гсха']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 39007\n",
      "в 9598\n",
      "и 9005\n",
      "на 4623\n",
      "с 3570\n",
      "по 2310\n",
      "год 2245\n",
      "для 1584\n",
      "от 1449\n",
      "что 1329\n",
      "при 1182\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "s_types = nltk.FreqDist(scientific_tokens)\n",
    "for t in s_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += s_types[t]\n",
    "        else:\n",
    "            lemmata[l] = s_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += s_types[t]\n",
    "        else:\n",
    "            lemmata[t] = s_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "год 2245\n",
      "пермский 1086\n",
      "который 962\n",
      "производство 888\n",
      "являться 812\n",
      "продукция 804\n",
      "развитие 727\n",
      "предприятие 681\n",
      "хозяйство 674\n",
      "сельскохозяйственный 651\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in scientific_stops:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### journalistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 4027\n",
      "в 713\n",
      "и 299\n",
      "на 243\n",
      "что 177\n",
      "с 174\n",
      "о 171\n",
      "год 138\n",
      "это 135\n",
      "по 135\n",
      "быть 106\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "j_types = nltk.FreqDist(journalistic_tokens)\n",
    "for t in j_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += j_types[t]\n",
    "        else:\n",
    "            lemmata[l] = j_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += j_types[t]\n",
    "        else:\n",
    "            lemmata[t] = j_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "journalistic_stops = stopwords.words('russian') + [u'это']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "год 138\n",
      "февраль 101\n",
      "россия 84\n",
      "который 78\n",
      "также 51\n",
      "российский 42\n",
      "заявить 42\n",
      "мочь 39\n",
      "сообщить 38\n",
      "стать 34\n",
      "процент 31\n",
      "время 31\n",
      "страна 29\n",
      "информация 29\n",
      "рост 29\n",
      "слово 28\n",
      "сообщать 28\n",
      "отметить 27\n",
      "рубль 25\n",
      "находиться 24\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in journalistic_stops:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подсчет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parts = ['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', \n",
    "         'INFN', 'PRTF', 'PRTS', 'GRND', 'NUMR', \n",
    "         'ADVB', 'NPRO', 'PRED', 'PREP', 'CONJ',\n",
    "         'PRCL', 'INTJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of verbs: 7030\n",
      "быть 2555\n",
      "хотеть 1117\n",
      "мочь 915\n",
      "любить 498\n",
      "знать 482\n",
      "думать 332\n",
      "сказать 293\n",
      "идти 275\n",
      "хотеться 250\n",
      "говорить 237\n",
      "пойти 230\n",
      "смотреть 230\n",
      "сидеть 216\n",
      "стать 201\n",
      "писать 199\n",
      "ждать 198\n",
      "болеть 195\n",
      "забыть 179\n",
      "понимать 177\n",
      "понять 176\n"
     ]
    }
   ],
   "source": [
    "verbs = nltk.FreqDist()\n",
    "for t in c_types:\n",
    "    gram_info = morph.parse(t)[0]\n",
    "    if 'VERB' in gram_info.tag:\n",
    "        l = gram_info.normal_form\n",
    "        if l in verbs:\n",
    "            verbs[l] += c_types[t]\n",
    "        else:\n",
    "            verbs[l] = c_types[t]\n",
    "print('N of verbs:', len(verbs))\n",
    "for i in verbs.most_common(20):\n",
    "    print( i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_parts_of_speech(types):\n",
    "    parts = ['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', \n",
    "         'INFN', 'PRTF', 'PRTS', 'GRND', 'NUMR', \n",
    "         'ADVB', 'NPRO', 'PRED', 'PREP', 'CONJ',\n",
    "         'PRCL', 'INTJ']\n",
    "    for p in parts:\n",
    "        words = nltk.FreqDist()\n",
    "        for t in types:\n",
    "            gram_info = morph.parse(t)[0]\n",
    "            if p in gram_info.tag:\n",
    "                l = gram_info.normal_form\n",
    "                if l in words:\n",
    "                    words[l] += types[t]\n",
    "                else:\n",
    "                    words[l] = types[t]\n",
    "        print('N of words:', len(words))\n",
    "        for i in words.most_common(10):\n",
    "            print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of words: 21192\n",
      "день 855\n",
      "человек 544\n",
      "год 492\n",
      "раз 461\n",
      "время 405\n",
      "дом 355\n",
      "друг 309\n",
      "работа 296\n",
      "утро 293\n",
      "жизнь 287\n",
      "N of words: 3371\n",
      "весь 3017\n",
      "мой 1227\n",
      "такой 1207\n",
      "один 867\n",
      "этот 853\n",
      "свой 662\n",
      "тот 608\n",
      "самый 472\n",
      "какой 401\n",
      "хороший 394\n",
      "N of words: 1733\n",
      "должный 120\n",
      "нужный 107\n",
      "пришлый 87\n",
      "нормальный 82\n",
      "холодный 81\n",
      "ладный 68\n",
      "какойтый 67\n",
      "обидный 66\n",
      "наконецтый 60\n",
      "пошлый 57\n",
      "N of words: 176\n",
      "большой 351\n",
      "хороший 189\n",
      "ранний 102\n",
      "далёкий 57\n",
      "маленький 49\n",
      "худой 46\n",
      "быстрый 37\n",
      "скорый 33\n",
      "близкий 31\n",
      "парный 22\n",
      "N of words: 7030\n",
      "быть 2555\n",
      "хотеть 1117\n",
      "мочь 915\n",
      "любить 498\n",
      "знать 482\n",
      "думать 332\n",
      "сказать 293\n",
      "идти 275\n",
      "хотеться 250\n",
      "говорить 237\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-14b5fa638337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_parts_of_speech\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-1c4e3c0ebf05>\u001b[0m in \u001b[0;36mcount_parts_of_speech\u001b[0;34m(types)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mgram_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgram_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgram_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliyetefikova/anaconda/lib/python3.5/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliyetefikova/anaconda/lib/python3.5/site-packages/pymorphy2/units/by_analogy.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word, word_lower, seen_parses)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mword_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mpara_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuffixes_dawg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mee\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfixed_suffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpara_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliyetefikova/anaconda/lib/python3.5/site-packages/dawg_python/dawgs.py\u001b[0m in \u001b[0;36msimilar_items\u001b[0;34m(self, key, replaces)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0municode\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \"\"\"\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_similar_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliyetefikova/anaconda/lib/python3.5/site-packages/dawg_python/dawgs.py\u001b[0m in \u001b[0;36m_similar_items\u001b[0;34m(self, current_prefix, key, index, replace_chars)\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mextra_items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollow_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliyetefikova/anaconda/lib/python3.5/site-packages/dawg_python/wrapper.py\u001b[0m in \u001b[0;36mfollow_bytes\u001b[0;34m(self, s, index)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m\"Follows transitions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollow_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_from_byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aliyetefikova/anaconda/lib/python3.5/site-packages/dawg_python/wrapper.py\u001b[0m in \u001b[0;36mfollow_char\u001b[0;34m(self, label, index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;34m\"Follows a transition\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m^\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m^\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRECISION_MASK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count_parts_of_speech(c_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
