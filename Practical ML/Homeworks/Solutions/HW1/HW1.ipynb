{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1:  Сравнение стилей текстов\n",
    "### Выполнили:  Булгаков Дмитрий, Тефикова Алие\n",
    "### Группа ИАД-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading data from file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составьте самостоятельно как минимум две коллекции\n",
    "текстов разных стилей (например, коллекция текстов в публицистическом\n",
    "стиле и коллекция текстов в научном стиле). Коллекции текстов\n",
    "должны быть достаточно большие (порядка 5000 токенов). Посчитайте\n",
    "количество токенов и типов в каждой коллекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_control_characters(text_string):\n",
    "    return ''.join(filter(None, text_string.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Reading fiction text from file (War and Peace by Leo Tolstoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "War and Peace (pre-reform Russian: Война́ и миръ; post-reform Russian: Война́ и мир, translit. Voyná i mir [vɐjˈna i ˈmʲir]) is a novel by the Russian author Leo Tolstoy, which is regarded as a central work of world literature and one of Tolstoy's finest literary achievements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link:<b> http://www.gutenberg.org/files/2600/2600-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fiction = open('data/wap.txt', encoding='utf-8').read()\n",
    "fiction = remove_control_characters(fiction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Parsing journalistic style texts from Returers article corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the most widely used test collection for text categorization research, though likely to be superceded over the next few years by RCV1.  The data was originally collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. in the course of developing the CONSTRUE text categorization system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link<b>: http://www.daviddlewis.com/resources/testcollections/reuters21578/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags_to_remove = ['date', 'topics', 'places', 'people', 'orgs', 'exchanges', 'companies', \n",
    "                  'unknown', 'title', 'dateline']\n",
    "reuters_stopwords = ['reuters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseXmlFile(path, tags, stopwords):\n",
    "    xml_soup = BeautifulSoup(open(path), 'lxml')\n",
    "    \n",
    "    for trash in xml_soup(tags):\n",
    "        trash.extract()\n",
    "        \n",
    "    parsed_text = xml_soup.get_text() # getting text from file \n",
    "    parsed_text = remove_control_characters(parsed_text) # removing control characters    \n",
    "    parsed_text_words = [word for word in parsed_text.split() if word.lower() not in stopwords] # removing stop words\n",
    "    return ' '.join(parsed_text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "journalistic = parseXmlFile('data/reuters.txt', tags=tags_to_remove, stopwords=reuters_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Reading scientific style texts from PhenoCHF  corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PhenoCHF - A corpus consisting of biomedical articles and clinincal records, annotated with phenotypic information related with congestive heart failure (CHF). Various levels of anonotation are included, i.e., entity mentions, their normalisation to concept IDs in the UMLS Metathesarus, and relations involving entity mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link<b>: http://www.nactem.ac.uk/phenotype/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scientific = open('data/phenoCHF.txt', encoding='utf-8').read()\n",
    "scientific = remove_control_characters(scientific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Reading conversational style texts from Skam, Shameless TV shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skam (Norwegian pronunciation: [skɑm]; English: shame) is a Norwegian young adult TV series about the daily life of teenagers at the Hartvig Nissen School (Hartvig Nissens skole), a gymnasium in the wealthy borough of Frogner in West End Oslo. It is produced by NRK P3, which is part of NRK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link:<b> https://drive.google.com/drive/folders/0Bxy61gL9aCrhQldNd0E4Vk8tWUk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conversational = open('data/skam_shameless.txt', encoding='utf-8').read()\n",
    "conversational = remove_control_characters(conversational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Counting tokens and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_symbols = set(punctuation + '0123456789'+u'–—'+u'«»'+u'“')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text, exlude_symb):\n",
    "    text = text.lower()\n",
    "    text_merged = ''.join(ch for ch in text if ch not in exlude_symb)\n",
    "    text_tokens = WhitespaceTokenizer().tokenize(text_merged.lower())\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(tokens):\n",
    "    print('N of tokens: ', len(tokens))\n",
    "    types = nltk.FreqDist(tokens)\n",
    "    print('N of types:', len(types))\n",
    "    print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fiction_tokens = tokenize(fiction, exclude_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well\n",
      "prince\n",
      "so\n",
      "genoa\n",
      "and\n",
      "lucca\n",
      "are\n",
      "now\n",
      "just\n",
      "family\n"
     ]
    }
   ],
   "source": [
    "for i in fiction_tokens[:10]: \n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  510360\n",
      "N of types: 56583\n",
      "<FreqDist with 56583 samples and 510360 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_results(fiction_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Journalistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "journalistic_tokens = tokenize(journalistic, exclude_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showers\n",
      "continued\n",
      "throughout\n",
      "the\n",
      "week\n",
      "inthe\n",
      "bahia\n",
      "cocoa\n",
      "zone\n",
      "alleviating\n"
     ]
    }
   ],
   "source": [
    "for i in journalistic_tokens[:10]: \n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  308598\n",
      "N of types: 34354\n",
      "<FreqDist with 34354 samples and 308598 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_results(journalistic_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scientific_tokens = tokenize(scientific, exclude_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "ventricular\n",
      "disease\n",
      "occurs\n",
      "frequentlyin\n",
      "dialysis\n",
      "patients\n",
      "it\n",
      "may\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "for i in scientific_tokens[:10]: \n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  32623\n",
      "N of types: 5222\n",
      "<FreqDist with 5222 samples and 32623 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_results(scientific_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conversational_tokens = tokenize(conversational, exclude_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "didnt\n",
      "think\n",
      "she\n",
      "was\n",
      "his\n",
      "typeneither\n",
      "did\n",
      "iwhen\n",
      "did\n"
     ]
    }
   ],
   "source": [
    "for i in conversational_tokens[:10]: \n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  270145\n",
      "N of types: 23778\n",
      "<FreqDist with 23778 samples and 270145 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_results(conversational_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Counting parts of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя любой морфологический процессор, которыйвам нравится (pymorphy2, mystem), определите к какой части речиотносятся слова из каждой коллекции текстов. При помощи nltk.FreqDist()составьте частотные словари: часть речи – количество слов, к нейотносящихся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "should\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "couldn\n",
      "didn\n",
      "doesn\n",
      "hadn\n",
      "hasn\n",
      "haven\n",
      "isn\n",
      "ma\n",
      "mightn\n",
      "mustn\n",
      "needn\n",
      "shan\n",
      "shouldn\n",
      "wasn\n",
      "weren\n",
      "won\n",
      "wouldn\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "for i in stopwords.words('english'):\n",
    "    print(i)\n",
    "mystopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 56583\n",
      "the 28554\n",
      "and 18642\n",
      "to 14574\n",
      "of 13021\n",
      "a 9085\n",
      "he 8335\n",
      "in 7623\n",
      "his 6770\n",
      "that 6620\n",
      "was 6347\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "f_types = nltk.FreqDist(fiction_tokens)\n",
    "for t in f_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += f_types[t]\n",
    "        else:\n",
    "            lemmata[l] = f_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += f_types[t]\n",
    "        else:\n",
    "            lemmata[t] = f_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said 2463\n",
      "one 1697\n",
      "prince 1383\n",
      "pierre 1214\n",
      "would 1157\n",
      "could 941\n",
      "man 867\n",
      "andrew 829\n",
      "went 769\n",
      "time 748\n",
      "natásha 722\n",
      "old 699\n",
      "know 688\n",
      "face 687\n",
      "french 681\n",
      "men 647\n",
      "eyes 633\n",
      "princess 630\n",
      "thought 625\n",
      "like 620\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 23778\n",
      "you 8331\n",
      "the 7514\n",
      "a 6606\n",
      "to 6151\n",
      "i 4950\n",
      "of 2836\n",
      "your 2361\n",
      "it 2304\n",
      "in 2249\n",
      "my 2232\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "c_types = nltk.FreqDist(conversational_tokens)\n",
    "for t in c_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += c_types[t]\n",
    "        else:\n",
    "            lemmata[l] = c_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += c_types[t]\n",
    "        else:\n",
    "            lemmata[t] = c_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 1615\n",
      "dont 1552\n",
      "know 1358\n",
      "got 1164\n",
      "im 1139\n",
      "like 1131\n",
      "want 1065\n",
      "youre 968\n",
      "go 878\n",
      "gonna 864\n",
      "need 801\n",
      "good 610\n",
      "cant 590\n",
      "think 585\n",
      "take 578\n",
      "come 558\n",
      "really 508\n",
      "one 504\n",
      "right 501\n",
      "going 494\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 5222\n",
      "of 1441\n",
      "the 1347\n",
      "in 1114\n",
      "and 1000\n",
      "with 643\n",
      "to 580\n",
      "a 519\n",
      "patients 476\n",
      "is 456\n",
      "for 319\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "s_types = nltk.FreqDist(scientific_tokens)\n",
    "for t in s_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += s_types[t]\n",
    "        else:\n",
    "            lemmata[l] = s_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += s_types[t]\n",
    "        else:\n",
    "            lemmata[t] = s_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients 476\n",
      "risk 250\n",
      "disease 223\n",
      "renal 210\n",
      "ckd 167\n",
      "cvd 163\n",
      "may 161\n",
      "heart 150\n",
      "failure 145\n",
      "dialysis 124\n",
      "associated 122\n",
      "esrd 119\n",
      "levels 117\n",
      "factors 116\n",
      "mortality 116\n",
      "increased 116\n",
      "studies 107\n",
      "also 103\n",
      "kidney 96\n",
      "cardiovascular 95\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### journalistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 34354\n",
      "the 16142\n",
      "of 8666\n",
      "to 8377\n",
      "said 6688\n",
      "and 6345\n",
      "a 6179\n",
      "in 6140\n",
      "mln 3658\n",
      "for 3190\n",
      "dlrs 2873\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english') + [u'mln', u'dlrs', u'vs', u'pct', u'cts', u'us', u'would']\n",
    "\n",
    "j_types = nltk.FreqDist(journalistic_tokens)\n",
    "for t in j_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += j_types[t]\n",
    "        else:\n",
    "            lemmata[l] = j_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += j_types[t]\n",
    "        else:\n",
    "            lemmata[t] = j_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said 6688\n",
      "year 1400\n",
      "billion 1257\n",
      "company 990\n",
      "net 974\n",
      "inc 842\n",
      "loss 840\n",
      "bank 765\n",
      "new 741\n",
      "corp 714\n",
      "also 653\n",
      "last 652\n",
      "one 643\n",
      "march 631\n",
      "sales 601\n",
      "share 593\n",
      "stock 570\n",
      "shares 556\n",
      "market 553\n",
      "profit 534\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
