{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pymorphy2\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1:  Сравнение стилей текстов\n",
    "### Выполнили:  Булгаков Дмитрий, Тефикова Алие\n",
    "### Группа ИАД-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составьте самостоятельно как минимум две коллекции\n",
    "текстов разных стилей (например, коллекция текстов в публицистическом\n",
    "стиле и коллекция текстов в научном стиле). Коллекции текстов\n",
    "должны быть достаточно большие (порядка 5000 токенов). Посчитайте\n",
    "количество токенов и типов в каждой коллекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Получение  данных из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_control_characters(text_string):\n",
    "    return ''.join(filter(None, text_string.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Загрузка художественных текстов (Портрет Дориана Грея,  Оскар Уайльд)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«Портре́т Дориана Гре́я» (англ. The Picture of Dorian Gray) — единственный опубликованный роман Оскара Уайльда. В жанровом отношении представляет смесь романа воспитания с моральной притчей. Существует в двух версиях — в 13 главах (1890 года) и в 20 главах (1891 года). Стал самым успешным произведением Уайльда, более 30 раз экранизировался."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link:<b> http://lib.ru/WILDE/doriangray.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fiction = open('data/dorian_gray.txt', encoding='utf-8').read()\n",
    "fiction = remove_control_characters(fiction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Загрузка  публицистических текстов (статьи lenta.ru) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenta.ru — одно из ведущих российских новостных интернет-изданий, основанное в 1999 году Антоном Носиком при содействии Фонда эффективной политики. Работает круглосуточно, освещая мировые и внутрироссийские новости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link<b>: http://lenta.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "journalistic = open('data/lentaru.txt', encoding='utf-8').read()\n",
    "journalistic = remove_control_characters(journalistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Загрузка  научных текстов (Молодежная Наука 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы Всероссийской научно-практической конференции молодых ученых, аспирантов и студентов, посвященной 150-летию со дня рождения профессора В.Н. Варгина (Пермь, 14-18 марта 2016 года)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link<b>: http://pgsha.ru/web/science/scientificarticles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scientific = open('data/perm_conf.txt', encoding='utf-8').read()\n",
    "scientific = remove_control_characters(scientific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Загрузка  текстов  разговорного стиля (корпус составленный на базе Twitter )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве источника текстов была выбрана платформа микроблогинга Twitter. Современные поисковые системы и имеющиеся в открытом доступе инструменты по сбору текстовых отзывов не позволяют собирать актуальные отзывы и оперативно работать с данными. В связи с этим на основе программного интерефейса API twitter  был разработан программный инструмент для извлечения отзывов об интересующих товарах, услугах,  событиях,  персонах из микроблоггинг-платформы twitter,  который позволяет учитывать время публикации сообщения и авторитетность автора сообщения. Этот инструмент использовался для сбора неразмеченного корпуса. В корпусе содержится более 15 миллионов записей за время с конца ноября 2013 года до конца февраля 2014 года."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link:<b> http://study.mokoron.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conversational = open('data/twitter.txt', encoding='utf-8').read()\n",
    "conversational = remove_control_characters(conversational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Подсчет токенов и типов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_symbols = set(punctuation + '0123456789'+u'–—'+u'«»'+u'“')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text, exlude_symb):\n",
    "    text = text.lower()\n",
    "    text_merged = ''.join(ch for ch in text if ch not in exlude_symb)\n",
    "    text_tokens = WhitespaceTokenizer().tokenize(text_merged.lower())\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(tokens):\n",
    "    print('N of tokens: ', len(tokens))\n",
    "    types = nltk.FreqDist(tokens)\n",
    "    print('N of types:', len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Токены и типы для  художественного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  60528\n",
      "N of types: 18236\n"
     ]
    }
   ],
   "source": [
    "fiction_tokens = tokenize(fiction, exclude_symbols)\n",
    "print_results(fiction_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Токены и типы для публицистического стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  13699\n",
      "N of types: 6015\n"
     ]
    }
   ],
   "source": [
    "journalistic_tokens = tokenize(journalistic, exclude_symbols)\n",
    "print_results(journalistic_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Токены и типы для  научного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  265376\n",
      "N of types: 57790\n"
     ]
    }
   ],
   "source": [
    "scientific_tokens = tokenize(scientific, exclude_symbols)\n",
    "print_results(scientific_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Токены и типы для  разговорного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  255367\n",
      "N of types: 56878\n"
     ]
    }
   ],
   "source": [
    "conversational_tokens = tokenize(conversational, exclude_symbols)\n",
    "print_results(conversational_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Подсчет частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя любой морфологический процессор, который вам нравится (pymorphy2, mystem), определите к какой части речиотносятся слова из каждой коллекции текстов. При помощи nltk.FreqDist() составьте частотные словари: часть речи – количество слов, к ней относящихся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mystopwords = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "f_types = nltk.FreqDist(fiction_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATN\n"
     ]
    }
   ],
   "source": [
    "print(morph.parse('picture')[0].tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "c_types = nltk.FreqDist(conversational_tokens)\n",
    "for t in c_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += c_types[t]\n",
    "        else:\n",
    "            lemmata[l] = c_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += c_types[t]\n",
    "        else:\n",
    "            lemmata[t] = c_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "s_types = nltk.FreqDist(scientific_tokens)\n",
    "for t in s_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += s_types[t]\n",
    "        else:\n",
    "            lemmata[l] = s_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += s_types[t]\n",
    "        else:\n",
    "            lemmata[t] = s_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### journalistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english') + [u'mln', u'dlrs', u'vs', u'pct', u'cts', u'us', u'would']\n",
    "\n",
    "j_types = nltk.FreqDist(journalistic_tokens)\n",
    "for t in j_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += j_types[t]\n",
    "        else:\n",
    "            lemmata[l] = j_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += j_types[t]\n",
    "        else:\n",
    "            lemmata[t] = j_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
