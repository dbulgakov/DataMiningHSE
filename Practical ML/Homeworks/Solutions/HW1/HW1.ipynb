{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pymorphy2\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1:  Сравнение стилей текстов\n",
    "### Выполнили:  Булгаков Дмитрий, Тефикова Алие\n",
    "### Группа ИАД-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составьте самостоятельно как минимум две коллекции\n",
    "текстов разных стилей (например, коллекция текстов в публицистическом\n",
    "стиле и коллекция текстов в научном стиле). Коллекции текстов\n",
    "должны быть достаточно большие (порядка 5000 токенов). Посчитайте\n",
    "количество токенов и типов в каждой коллекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Получение  данных из файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_control_characters(text_string):\n",
    "    return ''.join(filter(None, text_string.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Загрузка художественных текстов (Портрет Дориана Грея,  Оскар Уайльд)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "«Портре́т Дориана Гре́я» (англ. The Picture of Dorian Gray) — единственный опубликованный роман Оскара Уайльда. В жанровом отношении представляет смесь романа воспитания с моральной притчей. Существует в двух версиях — в 13 главах (1890 года) и в 20 главах (1891 года). Стал самым успешным произведением Уайльда, более 30 раз экранизировался."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link:<b> http://lib.ru/WILDE/doriangray.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fiction = open('data/dorian_gray.txt', encoding='utf-8').read()\n",
    "fiction = remove_control_characters(fiction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Загрузка  публицистических текстов (статьи lenta.ru) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenta.ru — одно из ведущих российских новостных интернет-изданий, основанное в 1999 году Антоном Носиком при содействии Фонда эффективной политики. Работает круглосуточно, освещая мировые и внутрироссийские новости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link<b>: http://lenta.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "journalistic = open('data/lentaru.txt', encoding='utf-8').read()\n",
    "journalistic = remove_control_characters(journalistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Загрузка  научных текстов (Молодежная Наука 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Материалы Всероссийской научно-практической конференции молодых ученых, аспирантов и студентов, посвященной 150-летию со дня рождения профессора В.Н. Варгина (Пермь, 14-18 марта 2016 года)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link<b>: http://pgsha.ru/web/science/scientificarticles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scientific = open('data/perm_conf.txt', encoding='utf-8').read()\n",
    "scientific = remove_control_characters(scientific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Загрузка  текстов  разговорного стиля (корпус составленный на базе Twitter )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве источника текстов была выбрана платформа микроблогинга Twitter. Современные поисковые системы и имеющиеся в открытом доступе инструменты по сбору текстовых отзывов не позволяют собирать актуальные отзывы и оперативно работать с данными. В связи с этим на основе программного интерефейса API twitter  был разработан программный инструмент для извлечения отзывов об интересующих товарах, услугах,  событиях,  персонах из микроблоггинг-платформы twitter,  который позволяет учитывать время публикации сообщения и авторитетность автора сообщения. Этот инструмент использовался для сбора неразмеченного корпуса. В корпусе содержится более 15 миллионов записей за время с конца ноября 2013 года до конца февраля 2014 года."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Link:<b> http://study.mokoron.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conversational = open('data/twitter.txt', encoding='utf-8').read()\n",
    "conversational = remove_control_characters(conversational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Подсчет токенов и типов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_symbols = set(punctuation + '0123456789'+u'–—'+u'«»'+u'“')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text, exlude_symb):\n",
    "    text = text.lower()\n",
    "    text_merged = ''.join(ch for ch in text if ch not in exlude_symb)\n",
    "    text_tokens = WhitespaceTokenizer().tokenize(text_merged.lower())\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_results(tokens):\n",
    "    print('N of tokens: ', len(tokens))\n",
    "    types = nltk.FreqDist(tokens)\n",
    "    print('N of types:', len(types))\n",
    "    print(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Токены и типы для  художественного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fiction_tokens = tokenize(fiction, exclude_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "художник\n",
      "тот\n",
      "кто\n",
      "создает\n",
      "прекрасное\n",
      "раскрыть\n",
      "людям\n",
      "себя\n",
      "и\n",
      "скрыть\n"
     ]
    }
   ],
   "source": [
    "for i in fiction_tokens[:10]: \n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  60528\n",
      "N of types: 18236\n",
      "<FreqDist with 18236 samples and 60528 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_results(fiction_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Токены и типы для публицистического стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "journalistic_tokens = tokenize(journalistic, exclude_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "на\n",
      "борту\n",
      "упавшего\n",
      "в\n",
      "телецкое\n",
      "озеро\n",
      "вертолета\n",
      "robinson\n",
      "находился\n",
      "бывший\n"
     ]
    }
   ],
   "source": [
    "for i in journalistic_tokens[:10]: \n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  13699\n",
      "N of types: 6015\n",
      "<FreqDist with 6015 samples and 13699 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_results(journalistic_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Токены и типы для  научного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scientific_tokens = tokenize(scientific, exclude_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "влияние\n",
      "соотношения\n",
      "компонентовна\n",
      "урожайность\n",
      "горохоячменной\n",
      "смесианнотация\n",
      "в\n",
      "работе\n",
      "представлены\n",
      "результаты\n"
     ]
    }
   ],
   "source": [
    "for i in scientific_tokens[:10]: \n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  32623\n",
      "N of types: 5222\n",
      "<FreqDist with 5222 samples and 32623 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_results(scientific_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Токены и типы для  разговорного стиля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conversational_tokens = tokenize(conversational, exclude_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "didnt\n",
      "think\n",
      "she\n",
      "was\n",
      "his\n",
      "typeneither\n",
      "did\n",
      "iwhen\n",
      "did\n"
     ]
    }
   ],
   "source": [
    "for i in conversational_tokens[:10]: \n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of tokens:  270145\n",
      "N of types: 23778\n",
      "<FreqDist with 23778 samples and 270145 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print_results(conversational_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Подсчет частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя любой морфологический процессор, который вам нравится (pymorphy2, mystem), определите к какой части речиотносятся слова из каждой коллекции текстов. При помощи nltk.FreqDist() составьте частотные словари: часть речи – количество слов, к ней относящихся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mystopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "f_types = nltk.FreqDist(fiction_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATN\n"
     ]
    }
   ],
   "source": [
    "print(morph.parse('picture')[0].tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said 2463\n",
      "one 1697\n",
      "prince 1383\n",
      "pierre 1214\n",
      "would 1157\n",
      "could 941\n",
      "man 867\n",
      "andrew 829\n",
      "went 769\n",
      "time 748\n",
      "natásha 722\n",
      "old 699\n",
      "know 688\n",
      "face 687\n",
      "french 681\n",
      "men 647\n",
      "eyes 633\n",
      "princess 630\n",
      "thought 625\n",
      "like 620\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 23778\n",
      "you 8331\n",
      "the 7514\n",
      "a 6606\n",
      "to 6151\n",
      "i 4950\n",
      "of 2836\n",
      "your 2361\n",
      "it 2304\n",
      "in 2249\n",
      "my 2232\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "c_types = nltk.FreqDist(conversational_tokens)\n",
    "for t in c_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += c_types[t]\n",
    "        else:\n",
    "            lemmata[l] = c_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += c_types[t]\n",
    "        else:\n",
    "            lemmata[t] = c_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 1615\n",
      "dont 1552\n",
      "know 1358\n",
      "got 1164\n",
      "im 1139\n",
      "like 1131\n",
      "want 1065\n",
      "youre 968\n",
      "go 878\n",
      "gonna 864\n",
      "need 801\n",
      "good 610\n",
      "cant 590\n",
      "think 585\n",
      "take 578\n",
      "come 558\n",
      "really 508\n",
      "one 504\n",
      "right 501\n",
      "going 494\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 5222\n",
      "of 1441\n",
      "the 1347\n",
      "in 1114\n",
      "and 1000\n",
      "with 643\n",
      "to 580\n",
      "a 519\n",
      "patients 476\n",
      "is 456\n",
      "for 319\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "s_types = nltk.FreqDist(scientific_tokens)\n",
    "for t in s_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += s_types[t]\n",
    "        else:\n",
    "            lemmata[l] = s_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += s_types[t]\n",
    "        else:\n",
    "            lemmata[t] = s_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients 476\n",
      "risk 250\n",
      "disease 223\n",
      "renal 210\n",
      "ckd 167\n",
      "cvd 163\n",
      "may 161\n",
      "heart 150\n",
      "failure 145\n",
      "dialysis 124\n",
      "associated 122\n",
      "esrd 119\n",
      "levels 117\n",
      "increased 116\n",
      "factors 116\n",
      "mortality 116\n",
      "studies 107\n",
      "also 103\n",
      "kidney 96\n",
      "cardiovascular 95\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### journalistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of lemmata: 34354\n",
      "the 16142\n",
      "of 8666\n",
      "to 8377\n",
      "said 6688\n",
      "and 6345\n",
      "a 6179\n",
      "in 6140\n",
      "mln 3658\n",
      "for 3190\n",
      "dlrs 2873\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemmata = nltk.FreqDist()\n",
    "\n",
    "mystopwords = stopwords.words('english') + [u'mln', u'dlrs', u'vs', u'pct', u'cts', u'us', u'would']\n",
    "\n",
    "j_types = nltk.FreqDist(journalistic_tokens)\n",
    "for t in j_types:\n",
    "    try:\n",
    "        l = morph.parse(t)[0].normal_form\n",
    "        if l in lemmata:\n",
    "            lemmata[l] += j_types[t]\n",
    "        else:\n",
    "            lemmata[l] = j_types[t]\n",
    "    except IndexError:\n",
    "        if t in lemmata:\n",
    "            lemmata[t] += j_types[t]\n",
    "        else:\n",
    "            lemmata[t] = j_types[t]\n",
    "print('N of lemmata:', len(lemmata))\n",
    "for i in lemmata.most_common(10):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said 6688\n",
      "year 1400\n",
      "billion 1257\n",
      "company 990\n",
      "net 974\n",
      "inc 842\n",
      "loss 840\n",
      "bank 765\n",
      "new 741\n",
      "corp 714\n",
      "also 653\n",
      "last 652\n",
      "one 643\n",
      "march 631\n",
      "sales 601\n",
      "share 593\n",
      "stock 570\n",
      "shares 556\n",
      "market 553\n",
      "profit 534\n"
     ]
    }
   ],
   "source": [
    "lemmata_no_sw = nltk.FreqDist()\n",
    "for l in lemmata:\n",
    "    if not l in mystopwords:\n",
    "        lemmata_no_sw[l] = lemmata[l]\n",
    "for i in lemmata_no_sw.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
